{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "    \"database\": \"job_market\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"root\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**db_params)\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_company(cur, company_data):\n",
    "    company = eval(company_data)\n",
    "    cur.execute(\"SELECT companyId FROM Companies WHERE companyName = %s;\", (company['name'],))\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cur.execute(\"INSERT INTO Companies (companyName, location, sector, information) VALUES (%s, %s, %s, %s) RETURNING companyId;\",\n",
    "                    (company['name'], company.get('location'), company.get('sector'), ''))\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "def get_or_create_source(cur, source_name):\n",
    "    cur.execute(\"SELECT sourceId FROM Sources WHERE sourceName = %s;\", (source_name,))\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cur.execute(\"INSERT INTO Sources (sourceName) VALUES (%s) RETURNING sourceId;\", (source_name,))\n",
    "        return cur.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"05 février 2024\" doesn't match format \"%d %B %Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([job_offer_adzuna, job_offer_wttj], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39mdf\u001b[38;5;241m.\u001b[39mcompany\u001b[38;5;241m.\u001b[39misna()]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m----> 8\u001b[0m \tstarting_date\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarting_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mB \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_date\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1108\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1108\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:254\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    252\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 254\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[1;32m    491\u001b[0m     arg,\n\u001b[1;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    509\u001b[0m     arg,\n\u001b[1;32m    510\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[0;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:355\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"05 février 2024\" doesn't match format \"%d %B %Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "\n",
    "job_offer_adzuna = pd.read_csv('../output/job_offers_adzuna.csv')\n",
    "job_offer_wttj = pd.read_csv('../output/job_offers_wttj.csv')\n",
    "\n",
    "df = pd.concat([job_offer_adzuna, job_offer_wttj], ignore_index=True)\n",
    "df = df[~df.company.isna()].reset_index(drop=True)\n",
    "\n",
    "df = df.assign(\n",
    "\tstarting_date=pd.to_datetime(df.starting_date, format='%d %B %Y')\n",
    ")\n",
    "df['starting_date'] = df['starting_date'].dt.date\n",
    "\n",
    "df['publication_date'] = pd.to_datetime(df['publication_date'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    company_id = get_or_create_company(cur, row['company'])\n",
    "    source_id = get_or_create_source(cur, row['source'])\n",
    "\n",
    "    job_offer_data = (\n",
    "        row['title'],\n",
    "        company_id,\n",
    "        row['salary'],\n",
    "        row['remote_type'],\n",
    "        row['contract_type'],\n",
    "        row['starting_date'] if not pd.isnull(row['starting_date']) else None,\n",
    "        row['location'],\n",
    "        row['required_experience'],\n",
    "        row['education'],\n",
    "        row['description'],\n",
    "        row['profil_experience'],\n",
    "        row['publication_date'].date() if not pd.isnull(row['publication_date']) else None,\n",
    "        row['url_direct_offer'],\n",
    "        source_id\n",
    "    )\n",
    "    cur.execute(\"INSERT INTO JobOffers (title, companyId, salary, remoteType, contractType, startingDate, location, requiredExp, education, descriptions, profilExp, publicationDate, jobLink, sourceId) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\", job_offer_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'API': 0, 'AWS': 0, 'Airflow': 0, 'Bash': 0, 'Cassandra': 0, 'Docker': 0, 'ElasticSearch': 0, 'FastAPI': 0, 'Flask': 0, 'Flume': 0, 'GCP': 0, 'Git': 0, 'Go': 0, 'Hadoop': 0, 'Hbase': 0, 'Hive': 0, 'Java': 0, 'Java Spark': 0, 'Julia': 0, 'Kafka': 0, 'Kotlin': 0, 'Kubernetes': 0, 'Matlib': 0, 'Microsoft Azure': 0, 'MongoDb': 0, 'Neo4j': 0, 'NoSQL': 0, 'Perl': 0, 'PySpark': 0, 'Python': 0, 'R': 0, 'Redshift': 0, 'SQL': 0, 'Scala': 0, 'SckitLearn': 0, 'Sklearn': 0, 'Snowflake': 0, 'Spark': 0, 'Spark Structured Streaming': 0, 'Terradata': 0}\n",
      "71\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# for the Skills table\n",
    "\n",
    "skills_list = ['API', 'AWS', 'Airflow', 'Bash', 'Cassandra', 'Docker', 'ElasticSearch', 'FastAPI', 'Flask', 'Flask', 'Flume', 'GCP', 'Git', 'Go', 'Hadoop', 'Hbase', 'Hive', 'Java', 'Java Spark', 'Julia', 'Kafka', 'Kotlin', 'Kubernetes', 'Matlib', 'Microsoft Azure', 'MongoDb', 'Neo4j', 'NoSQL', 'Perl', 'PySpark', 'Python', 'R', 'Redshift', 'SQL', 'Scala', 'SckitLearn', 'Sklearn', 'Snowflake', 'Spark', 'Spark Structured Streaming', 'Terradata']\n",
    "#skills_dict = dict.fromkeys(skills_list,0)\n",
    "#print(skills_dict)\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    description = (row['description'])\n",
    "    skill_data = []\n",
    "    for skill in skills_list:\n",
    "        if skill in description:\n",
    "            skill_data.append(skill)\n",
    "    cur.execute(\"INSERT INTO Skills (skills) VALUES (%s);\", skill_data)\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#print(type(str(df[\"description\"])))\n",
    "\n",
    "word_freq = Counter(str(df[\"description\"]).split()).most_common()\n",
    "print(len(word_freq))\n",
    "print(type(word_freq))\n",
    "\n",
    "\n",
    "df[\"description\"].to_csv('tmp_descriptions.csv', index=False)\n",
    "\n",
    "#for _, row in df.iterrows():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['API', 'AWS', 'Airflow', 'Bash', 'Cassandra', 'Docker', 'ElasticSearch', 'FastAPI', 'Flask', 'Flask', 'Flume', 'GCP', 'Git', 'Go', 'Hadoop', 'Hbase', 'Hive', 'Java', 'Java Spark', 'Julia', 'Kafka', 'Kotlin', 'Kubernetes', 'Matlib', 'Microsoft Azure', 'MongoDb', 'Neo4j', 'NoSQL', 'Perl', 'PySpark', 'Python', 'R', 'Redshift', 'SQL', 'Scala', 'SckitLearn', 'Sklearn', 'Snowflake', 'Spark', 'Spark Structured Streaming', 'Terradata']\n"
     ]
    }
   ],
   "source": [
    "list_to_sort = [\"SQL\",\n",
    "\"Bash\",\n",
    "\"Git\",\n",
    "\"NoSQL\",\n",
    "\"Redshift\",\n",
    "\"Terradata\",\n",
    "\"Cassandra\",\n",
    "\"Spark\",\n",
    "\"Hadoop\",\n",
    "\"Kafka\",\n",
    "\"Hbase\",\n",
    "\"Hive\",\n",
    "\"Microsoft Azure\",\n",
    "\"AWS\",\n",
    "\"GCP\",\n",
    "\"Python\",\n",
    "\"Java\",\n",
    "\"Java Spark\",\n",
    "\"PySpark\",\n",
    "\"Go\",\n",
    "\"Scala\",\n",
    "\"Julia\",\n",
    "\"Perl\",\n",
    "\"MongoDb\",\n",
    "\"ElasticSearch\",\n",
    "\"Flume\",\n",
    "\"Docker\",\n",
    "\"API\",\n",
    "\"Flask\",\n",
    "\"Airflow\",\n",
    "\"Kubernetes\",\n",
    "\"SckitLearn\",\n",
    "\"Kotlin\",\n",
    "\"Spark Structured Streaming\",\n",
    "\"R\",\n",
    "\"Neo4j\",\n",
    "\"Sklearn\",\n",
    "\"Matlib\",\n",
    "\"FastAPI\",\n",
    "\"Flask\",\n",
    "\"Snowflake\"]\n",
    "print(sorted(list_to_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#couper la connexion\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
