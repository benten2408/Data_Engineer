{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouverture de la connexion avec postgres\n",
    "\n",
    "db_params = {\n",
    "    \"database\": \"job_market\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"root\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**db_params)\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# création du dataframe complet\n",
    "\n",
    "job_offer_adzuna = pd.read_csv('../output/job_offers_adzuna.csv')\n",
    "job_offer_wttj = pd.read_csv('../output/job_offers_wttj.csv')\n",
    "\n",
    "df = pd.concat([job_offer_adzuna, job_offer_wttj], ignore_index=True)\n",
    "\n",
    "# nettoyage rapide\n",
    "\n",
    "df = df[~df.company.isna()].reset_index(drop=True)\n",
    "\n",
    "#print(f\"type of df.starting_date {type(df.starting_date)} et df.starting_date {df.starting_date} \\n\")\n",
    "\n",
    "df = df.assign(\n",
    "\t#starting_date=pd.to_datetime(df.starting_date,dayfirst=True)\n",
    "    starting_date=pd.to_datetime(df['starting_date'], format='%d %B %Y')\n",
    ")\n",
    "df['starting_date'] = df['starting_date'].dt.date\n",
    "\n",
    "df['publication_date'] = pd.to_datetime(df['publication_date'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "df = df.where(pd.notnull(df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agile', 'airbyte', 'airflow', 'amazon s3', 'amplitude', 'anglais', 'ansible', 'api', 'athena', 'aws', 'aws athena', 'aws glue', 'aws lambda', 'aws s3', 'azure data factory', 'azure databricks', 'bases de données relationnelles', 'bash', 'beam', 'big query', 'blob', 'braze', 'cassandra', 'ci/cd', 'clickhouse', 'cloud functions', 'cloudera', 'code idempotency', 'code quality', 'concourse', 'confluence', 'continuous delivery', 'dash', 'dask', 'data/business intelligence (bi)', 'databricks', 'dataflow', 'dataiku', 'datamesh', 'dataops', 'datastage', 'datastudio', 'dbt', 'denodo', 'devops', 'docker', 'dremio', 'dynamodb', 'elastic compute cloud (ec2)', 'elastic container service (ecs)', 'elastic kubernetes service', 'elasticsearch', 'elk', 'elt', 'fastapi', 'flask', 'flink', 'flume', 'gcp', 'gcp bigquery', 'gcp cloud build', 'gcp cloud run', 'git', 'gitlab', 'glue', 'go', 'google bigquery', 'gradle', 'hadoop', 'hbase', 'hdfs', 'helm', 'hive', 'hudi', 'impala', 'influxdb', 'intellij', 'java', 'java spark', 'javascript', 'jenkins', 'jira', 'julia', 'jupyter', 'k8s', 'kafka', 'kanban', 'kedro', 'kotlin', 'kubernetes', 'lake-house architecture', 'langchain', 'lightgbm', 'linux', 'llama index', 'llm', 'looker', 'looker studio', 'machine learning', 'mapr', 'matlib', 'maven', 'microsoft azure', 'mlflow', 'mongodb', 'multi threading', 'méthodologie agile', 'neo4j', 'nifi', 'nlp', 'nosql', 'openshift', 'oracle', 'outscale', 'ovh', 'perl', 'postgres', 'postgresql', 'power bi ', 'pyspark', 'pytest', 'python', 'qlik', 'qliksense', 'r-shiny', 'rabbitmq', 'redis', 'redshift', 'remoulade', 'rstudio', 'safe', 'scala', 'sckitlearn', 'shell', 'singlestore', 'sklearn', 'snaplogic', 'snowflake', 'spark', 'spark structured streaming', 'sprint planning', 'sql', 'sqlalchemy', 'star schéma', 'tableau', 'talend', 'tensorflow', 'terradata', 'terraform', 'test coverage', 'travis', 'ubuntu', 'vertexai', 'virtualisation', 'virtualisation bare-metal avancées', 'xgbooost']\n",
      "156\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# for the Skills table\n",
    "# attention au R pour qu'il ne soit pas confondu avec une majuscule\n",
    "\n",
    "df_skills = pd.read_csv('./skills.csv')\n",
    "skills_list = df_skills.iloc[:, 0].tolist()\n",
    "#print(list(skills_list))\n",
    "#print(len(skills_list))\n",
    "#print(type(skills_list))\n",
    "\n",
    "for skill in skills_list:\n",
    "    cur.execute(\"SELECT skillName FROM Skills WHERE skillName = %s;\", (skill,))\n",
    "    result = cur.fetchone()\n",
    "    if result is None:\n",
    "        cur.execute(\"INSERT INTO Skills (skillName) VALUES (%s);\", [skill])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AVANT \n",
    "for the JobOffer_Skills table\n",
    "2 possiblités : \n",
    "    soit une ligne par skillId (et donc plusieurs lignes pour le même jobId) \n",
    "    soit une ligne par jobId  ligne (et donc plusieurs pour le même skillId)\n",
    "Un offre d’emploi peut être associée à plusieurs compétences et vice versa.\n",
    "\n",
    "pour rappel\n",
    "CREATE TABLE JobOffer_Skills (\n",
    "        jobOfferId INTEGER REFERENCES JobOffers,\n",
    "        skillId INTEGER REFERENCES Skills(skillId),\n",
    "        PRIMARY KEY (jobOfferId, skillId)\n",
    "    );\n",
    "\n",
    "\n",
    "POURQUOI PAS ajouter skillId dans JOboffers directement ?\n",
    "\"\"\"\n",
    "def get_or_create_skill(cur, description):\n",
    "    skillids_in_offer = []\n",
    "    print(description, \"\\n\")\n",
    "    for skill in skills_list:\n",
    "        if skill in description:\n",
    "            print(f\"OMG_____   skill {skill}____________\\n\\n\\n\\n\\n\")\n",
    "            cur.execute(\"SELECT skillId FROM Skills WHERE skillName = %s;\", [skill])\n",
    "            result = cur.fetchone()\n",
    "            skillids_in_offer.append(result[0])\n",
    "    return [skillids_in_offer]\n",
    "    \"\"\"if result:\n",
    "            # il faut lier le skillId (où je le trouve ?) au JobId (où je le trouve ?)\n",
    "            cur.execute(\"INSERT INTO JobOffer_Skills (sourceName) VALUES (%s);\", (source_name,))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_company(cur, company_data):\n",
    "    company = eval(company_data)\n",
    "    cur.execute(\"SELECT companyId FROM Companies WHERE companyName = %s;\", (company['name'],))\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cur.execute(\"INSERT INTO Companies (companyName, location, sector, information) VALUES (%s, %s, %s, %s) RETURNING companyId;\",\n",
    "                    (company['name'], company.get('location'), company.get('sector'), ''))\n",
    "        return cur.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_source(cur, source_name):\n",
    "    cur.execute(\"SELECT sourceId FROM Source WHERE sourceName = %s;\", (source_name,))\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cur.execute(\"INSERT INTO Source (sourceName) VALUES (%s) RETURNING sourceId;\", (source_name,))\n",
    "        return cur.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    company_id = get_or_create_company(cur, row['company'])\n",
    "    source_id = get_or_create_source(cur, row['source'])\n",
    "    skill_id = get_or_create_skill(cur, row['description'])\n",
    "\n",
    "    job_offer_data = (\n",
    "        row['title'],\n",
    "        company_id,\n",
    "        row['salary'],\n",
    "        row['remote_type'],\n",
    "        row['contract_type'],\n",
    "        row['starting_date'] if not pd.isnull(row['starting_date']) else None,\n",
    "        row['location'],\n",
    "        row['require_experience'],\n",
    "        row['education'],\n",
    "        row['description'],\n",
    "        row['profil_experience'],\n",
    "        row['publication_date'].date() if not pd.isnull(row['publication_date']) else None,\n",
    "        row['url_direct_offer'],\n",
    "        source_id,\n",
    "        skill_id\n",
    "    )\n",
    "    cur.execute(\"INSERT INTO JobOffers (title, companyId, salary, remoteType, contractType, startingDate, location, requiredExp, education, descriptions, profilExp, publicationDate, jobLink, sourceId, skillId) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\", job_offer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agile', 'airbyte', 'airflow', 'amazon s3', 'amplitude', 'anglais', 'ansible', 'api', 'athena', 'aws', 'aws athena', 'aws glue', 'aws lambda', 'aws s3', 'azure data factory', 'azure databricks', 'bases de données relationnelles', 'bash', 'beam', 'big query', 'blob', 'braze', 'cassandra', 'ci/cd', 'clickhouse', 'cloud functions', 'cloudera', 'code idempotency', 'code quality', 'concourse', 'confluence', 'continuous delivery', 'dash', 'dask', 'data/business intelligence (bi)', 'databricks', 'dataflow', 'dataiku', 'datamesh', 'dataops', 'datastage', 'datastudio', 'dbt', 'denodo', 'devops', 'docker', 'dremio', 'dynamodb', 'elastic compute cloud (ec2)', 'elastic container service (ecs)', 'elastic kubernetes service', 'elasticsearch', 'elk', 'elt', 'fastapi', 'flask', 'flink', 'flume', 'gcp', 'gcp bigquery', 'gcp cloud build', 'gcp cloud run', 'git', 'gitlab', 'glue', 'go', 'google bigquery', 'gradle', 'hadoop', 'hbase', 'hdfs', 'helm', 'hive', 'hudi', 'impala', 'influxdb', 'intellij', 'java', 'java spark', 'javascript', 'jenkins', 'jira', 'julia', 'jupyter', 'k8s', 'kafka', 'kanban', 'kedro', 'kotlin', 'kubernetes', 'lake-house architecture', 'langchain', 'lightgbm', 'linux', 'llama index', 'llm', 'looker', 'looker studio', 'machine learning', 'mapr', 'matlib', 'maven', 'microsoft azure', 'mlflow', 'mongodb', 'multi threading', 'méthodologie agile', 'neo4j', 'nifi', 'nlp', 'nosql', 'openshift', 'oracle', 'outscale', 'ovh', 'perl', 'postgres', 'postgresql', 'power bi ', 'pyspark', 'pytest', 'python', 'qlik', 'qliksense', 'r-shiny', 'rabbitmq', 'redis', 'redshift', 'remoulade', 'rstudio', 'safe', 'scala', 'sckitlearn', 'shell', 'singlestore', 'sklearn', 'snaplogic', 'snowflake', 'spark', 'spark structured streaming', 'sprint planning', 'sql', 'sqlalchemy', 'star schéma', 'tableau', 'talend', 'tensorflow', 'terradata', 'terraform', 'test coverage', 'travis', 'ubuntu', 'vertexai', 'virtualisation', 'virtualisation bare-metal avancées', 'xgbooost']\n",
      "156\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Direction des flèches: il y a deux relations impliquées ici\n",
    "JobOffer vers JobOffer_Skills\n",
    "Skill vers JobOffer_Skills\n",
    "Signification : Chaque flèche pointe vers JobOffer_Skills, indiquant qu’elle sert de table de jonction. \n",
    "Il n’y a pas de flèche directe entre JobOffer et Skills parce que leur relation est médiée à travers JobOffer_Skills. \n",
    "Un offre d’emploi peut être associée à plusieurs compétences et vice versa.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#couper la connexion\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
